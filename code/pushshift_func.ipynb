{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here. Collect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushshift Reddit API and the two subreddits to collect data for\n",
    "url = 'https://api.pushshift.io/reddit/search/'\n",
    "subreddits = ['interestingasfuck', 'mildlyinteresting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TAKEN FROM STACKOVERFLOW. USER GTOWNROWER\n",
    "# https://stackoverflow.com/questions/41420742/python-repeatedly-writing-to-a-csv-file-from-a-dictionary-with-header-equal-to\n",
    "\n",
    "# This function appends a dictionary to a csv file. If there are no headers in the file, they are added. If not, just the rows (values of the keys) will be added\n",
    "def dict_to_csv(post_dict, file): \n",
    "    with open(f'../data/{file}.csv', 'a', encoding='utf-8') as f:\n",
    "        w = csv.DictWriter(f, post_dict.keys())\n",
    "        if f.tell() == 0:\n",
    "            w.writeheader()\n",
    "            w.writerow(post_dict)\n",
    "        else: \n",
    "            w.writerow(post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to gather data\n",
    "# The function takes in:\n",
    "#   A list of subreddits to gather data on\n",
    "#   What kind of data to pull (submission or comment)\n",
    "#   How many cycles the function will run for (pulls 100 entries each cycle)\n",
    "# The function writes to a csv files and returns a dataframe\n",
    "def pushshift_query(subreddits, kind, cycles):\n",
    "    # checks to see if there are any entries in the csv file. \n",
    "    # If there are not, time is blank. If there are entries, the current_time is set to the last entry to be able to continue where it left off\n",
    "    if os.stat(f'../data/{kind}.csv').st_size == 0:\n",
    "        current_time = ''\n",
    "    else:\n",
    "        current_time = pd.read_csv(f'../data/{kind}.csv')['time'].min()\n",
    "\n",
    "    posts = []\n",
    "    title_or_body = 'title'\n",
    "\n",
    "    # switches between 'title' and 'body', based on what 'kind' was entered\n",
    "    if kind == 'comment':\n",
    "        title_or_body = 'body'\n",
    "\n",
    "    # iterates over the subreddit list\n",
    "    for subreddit in subreddits:\n",
    "        # for each subreddit, pulls data for the amount of 'cycles' that was input\n",
    "        for _ in range(cycles):\n",
    "            # params that get updated and injected into the url\n",
    "            params = {\n",
    "                'subreddit': subreddit,\n",
    "                'size': 100,\n",
    "                'before': current_time\n",
    "            }\n",
    "\n",
    "            # sends a request to the API\n",
    "            res = requests.get(url + kind, params)\n",
    "\n",
    "            # Checks the response. If not 200, exit function with error message\n",
    "            if res.status_code != 200:\n",
    "                return 'Error Occurred'\n",
    "\n",
    "            # iterates over all the entries for this cycle\n",
    "            # gathers the subreddit, title or body, auth, and time. adds them to a dictionary\n",
    "            for post in res.json()['data']:\n",
    "                post_dict = {}\n",
    "\n",
    "                post_dict['subreddit'] = post['subreddit']\n",
    "                post_dict[title_or_body] = post[title_or_body]\n",
    "                post_dict['auth'] = post['author']\n",
    "                post_dict['time'] = post['created_utc']\n",
    "\n",
    "                posts.append(post_dict)\n",
    "\n",
    "                # calls the function to append a dictionary to a csv\n",
    "                dict_to_csv(post_dict, kind)\n",
    "\n",
    "                # sets the last entries time to the current_time, to continue pulling data in sequential order\n",
    "                current_time = pd.DataFrame(posts)['time'].min()\n",
    "\n",
    "            print(f'Current data frame has {len(posts)} rows')\n",
    "            # sleeps for 3 seconds to let the servers rest\n",
    "            time.sleep(3)\n",
    "\n",
    "    # returns a data frame of all the posts collected for this function call\n",
    "    return pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pushshift_query(subreddits, 'comment', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pushshift_query(subreddits, 'submission', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go to the file \"comment_cleaning.ipynb\" next to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
