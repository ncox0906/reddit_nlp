{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This files cleans the data. Collect data first in \"pushshift_func.ipynb\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = pd.read_csv('../data/comment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116443 entries, 0 to 116442\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   subreddit  116443 non-null  object\n",
      " 1   body       116443 non-null  object\n",
      " 2   auth       116443 non-null  object\n",
      " 3   time       116443 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_comment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mildlyinteresting    58355\n",
       "interestingasfuck    58088\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicates, Deleted/Removed, and Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes comments with urls\n",
    "df_comment = df_comment[df_comment['body'].str.contains('http') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes comments that either have the author or comment deleted\n",
    "# removes all automoderator posts\n",
    "df_comment = df_comment[df_comment['auth'] != '[deleted]']\n",
    "df_comment = df_comment[df_comment['body'] != '[removed]']\n",
    "df_comment = df_comment[df_comment['auth'] != 'AutoModerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes comments of users asking to save a video\n",
    "df_comment = df_comment[df_comment['body'].str.contains('savevideo') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of authors that were for sure bots. There will be more as more data is added most likely\n",
    "bots = ['reply-guy-bot', 'savevideobot', 'stabbot', 'SaveVideo', 'fakefakebotdetective']\n",
    "\n",
    "# removes comments of bots listed above\n",
    "df_comment = df_comment[df_comment['auth'].isin(bots) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>auth</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>Well that's a weird super power</td>\n",
       "      <td>thewholerobot</td>\n",
       "      <td>1642803964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>You're definitely creepy</td>\n",
       "      <td>thewholerobot</td>\n",
       "      <td>1642802325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>Are you sitting nekkid on your phone maybe?</td>\n",
       "      <td>thewholerobot</td>\n",
       "      <td>1642802275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>Eye balls</td>\n",
       "      <td>thewholerobot</td>\n",
       "      <td>1642802248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>No no no!  This is reddit and we must dedicate...</td>\n",
       "      <td>TinFoilRobotProphet</td>\n",
       "      <td>1642800741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113808</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>Unless you're a municipality than profit away!...</td>\n",
       "      <td>kelvin_klein_bottle</td>\n",
       "      <td>1642108225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113946</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>The thing is he could jerk off in front of me ...</td>\n",
       "      <td>Spacbot</td>\n",
       "      <td>1642106999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114232</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>How is your shorter hallway a raise?</td>\n",
       "      <td>cudntbebothered</td>\n",
       "      <td>1642104589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114504</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>Naw. Doesn't even peak for another 100years bruh.</td>\n",
       "      <td>thewholerobot</td>\n",
       "      <td>1642102462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114613</th>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>Eat it.</td>\n",
       "      <td>botaine</td>\n",
       "      <td>1642101519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                subreddit                                               body  \\\n",
       "915     mildlyinteresting                    Well that's a weird super power   \n",
       "1155    mildlyinteresting                           You're definitely creepy   \n",
       "1163    mildlyinteresting        Are you sitting nekkid on your phone maybe?   \n",
       "1167    mildlyinteresting                                          Eye balls   \n",
       "1543    interestingasfuck  No no no!  This is reddit and we must dedicate...   \n",
       "...                   ...                                                ...   \n",
       "113808  mildlyinteresting  Unless you're a municipality than profit away!...   \n",
       "113946  mildlyinteresting  The thing is he could jerk off in front of me ...   \n",
       "114232  mildlyinteresting               How is your shorter hallway a raise?   \n",
       "114504  mildlyinteresting  Naw. Doesn't even peak for another 100years bruh.   \n",
       "114613  mildlyinteresting                                            Eat it.   \n",
       "\n",
       "                       auth        time  \n",
       "915           thewholerobot  1642803964  \n",
       "1155          thewholerobot  1642802325  \n",
       "1163          thewholerobot  1642802275  \n",
       "1167          thewholerobot  1642802248  \n",
       "1543    TinFoilRobotProphet  1642800741  \n",
       "...                     ...         ...  \n",
       "113808  kelvin_klein_bottle  1642108225  \n",
       "113946              Spacbot  1642106999  \n",
       "114232      cudntbebothered  1642104589  \n",
       "114504        thewholerobot  1642102462  \n",
       "114613              botaine  1642101519  \n",
       "\n",
       "[207 rows x 4 columns]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment[df_comment['auth'].str.contains('bot')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing new line chars and first round of stripping whitespace\n",
    "df_comment = df_comment.replace([r'\\n', r'\\r'],' ', regex=True)\n",
    "df_comment['body'] = df_comment['body'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding subreddits\n",
    "# interestingasfuck = 1\n",
    "# mildlyinteresting = 0\n",
    "df_comment['subreddit'] = df_comment['subreddit'].map({'interestingasfuck':1, 'mildlyinteresting':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting number of chars and number of words per comment. assign to new feature\n",
    "df_comment['num_of_chars'] = df_comment['body'].map(lambda x: len(x))\n",
    "df_comment['word_count'] = df_comment['body'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of capital letters in each comment. assign to feature\n",
    "def capital_letters(post):\n",
    "    count = 0\n",
    "    for letter in post:\n",
    "        if letter.isupper():\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "df_comment['capital_count'] = df_comment['body'].map(capital_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if ?, !, or ... are in a comment. if so, assign a 1, if not assign 0\n",
    "df_comment['question_mark'] = df_comment['body'].map(lambda x: 1 if '?' in x else 0)\n",
    "df_comment['exclaimation'] = df_comment['body'].map(lambda x: 1 if '!' in x else 0)\n",
    "df_comment['dot_dot_dot'] = df_comment['body'].map(lambda x: 1 if '...' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of regex to see if a comment has text in quotes. accounts for multiple ways a user would use quotes\n",
    "quotes = [\n",
    "    '^[\"]{1}[^\"]+[\"]{1}$',\n",
    "    '[^\"][\"]{1}[^\"]+[\"]{1}[^\"]',\n",
    "    '[^\"]?[\"]{1}[^\"]+[\"]{1}[^\"]',\n",
    "    '[^\"][\"]{1}[^\"]+[\"]{1}[^\"]?'\n",
    "]\n",
    "\n",
    "# function to apply regex pattern and check if it's found. if so, assign 1. otherwise 0\n",
    "def find_quotes(comment):\n",
    "    if re.findall(quotes[0], comment) or re.findall(quotes[1], comment) or re.findall(quotes[2], comment) or re.findall(quotes[3], comment):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_comment['quotes'] = df_comment['body'].map(find_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of regex to see if a comment has text in italics. accounts for multiple ways a user would use italics\n",
    "italics = [\n",
    "    '^[*]{1}[^*]+[*]{1}$',\n",
    "    '[^*][*]{1}[^*]+[*]{1}[^*]',\n",
    "    '[^*]?[*]{1}[^*]+[*]{1}[^*]',\n",
    "    '[^*][*]{1}[^*]+[*]{1}[^*]?'\n",
    "]\n",
    "\n",
    "# function to apply regex pattern and check if it's found. if so, assign 1. otherwise 0\n",
    "def find_italics(comment):\n",
    "    if re.findall(italics[0], comment) or re.findall(italics[1], comment) or re.findall(italics[2], comment) or re.findall(italics[3], comment):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_comment['italics'] = df_comment['body'].map(find_italics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of regex to see if a comment has text in bold. accounts for multiple ways a user would use bold\n",
    "bold = [\n",
    "    '^[*]{2}[^*]+[*]{2}$',\n",
    "    '[^*][*]{2}[^*]+[*]{2}[^*]',\n",
    "    '[^*]?[*]{2}[^*]+[*]{2}[^*]',\n",
    "    '[^*][*]{2}[^*]+[*]{2}[^*]?'\n",
    "]\n",
    "\n",
    "# function to apply regex pattern and check if it's found. if so, assign 1. otherwise 0\n",
    "def find_bold(comment):\n",
    "    if re.findall(bold[0], comment) or re.findall(bold[1], comment) or re.findall(bold[2], comment) or re.findall(bold[3], comment):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_comment['bold'] = df_comment['body'].map(find_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TAKEN FROM CLASS LESSON\n",
    "def detect_polarity(text):\n",
    "    '''Accepts text and returns the polarity'''\n",
    "    \n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TAKEN FROM CLASS LESSON\n",
    "def detect_subjectivity(text):\n",
    "    '''Accepts text and returns the subjectivity'''\n",
    "    \n",
    "    return TextBlob(text).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies functions to get polarity and subjectivity of the comment\n",
    "df_comment['polarity'] = df_comment['body'].map(detect_polarity)\n",
    "df_comment['subjectivity'] = df_comment['body'].map(detect_subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns all text into lowercase\n",
    "df_comment['body'] = df_comment['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unicode apostrophe with normal one. will account for more contractions to expand\n",
    "df_comment['body'] = df_comment['body'].map(lambda x: x.replace(\"’\", \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "# contraction diction taken from stackoverflow. will be used to expand contractions in text\n",
    "\n",
    "contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns contraction keys into a list to iterate over\n",
    "contractions_list = list(contractions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "# Learned this and took it from stackoverflow. Author username is Brian\n",
    "# strips all punctuation\n",
    "\n",
    "df_comment['body'] = df_comment['body'].map(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to expand contractions\n",
    "def expand_contractions(post):\n",
    "    expanded_sentence = []\n",
    "\n",
    "    for word in post.split():\n",
    "        if word.lower() in contractions_list:\n",
    "            word = contractions[word.lower()]\n",
    "\n",
    "        expanded_sentence.append(word)\n",
    "\n",
    "    return ' '.join(expanded_sentence)\n",
    "\n",
    "df_comment['body'] = df_comment['body'].map(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all \"'s\" from the end of words\n",
    "df_comment['body'] = df_comment['body'].map(lambda x: x.replace(\"'s\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all comments that just have a space or are blank\n",
    "df_comment = df_comment[df_comment['body'] != ' ']\n",
    "df_comment = df_comment[df_comment['body'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the average length of a word for a comment\n",
    "# this feature had to be engineered after the cleaning above was done\n",
    "def get_avg_word_length(post):\n",
    "    word_list = []\n",
    "\n",
    "    for word in post.split():\n",
    "        word_list.append(len(word))\n",
    "\n",
    "    return round(np.mean(word_list))\n",
    "\n",
    "df_comment['avg_word_length'] = df_comment['body'].map(get_avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of custom stop words\n",
    "custom_stop_words = [\n",
    "    \"yep\",\n",
    "    \"yes\",\n",
    "    \"yup\",\n",
    "    \"no\",\n",
    "    \"nope\",\n",
    "    \"na\",\n",
    "    \"lol\",\n",
    "    \"idk\",\n",
    "    \"hi\",\n",
    "    \"yo\",\n",
    "    \"ok\",\n",
    "    \"ha\",\n",
    "    \"le\",\n",
    "    \"wa\",\n",
    "    \"nt\",\n",
    "    \"dont\",\n",
    "    \"youre\",\n",
    "    \"thats\",\n",
    "    \"got\",\n",
    "    \"im\",\n",
    "    \"ive\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    'h',\n",
    "    'i',\n",
    "    'j',\n",
    "    'k',\n",
    "    'l',\n",
    "    'm',\n",
    "    'n',\n",
    "    'o',\n",
    "    'p',\n",
    "    'q',\n",
    "    'r',\n",
    "    's',\n",
    "    't',\n",
    "    'u',\n",
    "    'v',\n",
    "    'w',\n",
    "    'x',\n",
    "    'y',\n",
    "    'z',\n",
    "    'like'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines nltk's stopwords with my custom list\n",
    "stop_words = stopwords.words('english') + custom_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the stop words of a comment\n",
    "# this feature had to be engineered after the cleaning above was done\n",
    "def stop_word_count(post):\n",
    "    count = 0\n",
    "    for word in post.split():\n",
    "        if word.lower().strip() in stop_words:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "df_comment['stop_word_count'] = df_comment['body'].map(stop_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stop words\n",
    "def stop_word_remove(post):\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for word in post.split():\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            filtered_sentence.append(word)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "df_comment['body'] = df_comment['body'].map(stop_word_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes anything except letters and spaces\n",
    "df_comment['body'] = df_comment['body'].map(lambda x: re.sub(\"[^A-Za-z ]\", \" \", x))\n",
    "\n",
    "# removes excessive whitespace and strips whitespace\n",
    "df_comment['body'] = df_comment['body'].map(lambda x: re.sub('\\s+', ' ', x))\n",
    "df_comment['body'] = df_comment['body'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes posts that are just a space or have nothing\n",
    "df_comment = df_comment[df_comment['body'] != ' ']\n",
    "df_comment = df_comment[df_comment['body'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops auth and time columns because they wont be used\n",
    "df_comment.drop(columns=['auth', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves cleaned data to new csv\n",
    "df_comment.to_csv('../data/comment_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is now clean and can be modelled in \"models.ipynb\" or perform EDA in \"eda.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
